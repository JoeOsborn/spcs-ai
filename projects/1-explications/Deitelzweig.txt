Recently, Microsoft created a bot on Twitter that would learn from the tweets of others in order to make its own tweets. The bot would mimic other users and have conversations with them as well. It was to be the first of its kind, seeing as most Twitter bots have built-in responses that do not learn in any way. It was likely to be used as just an experiment in conventional understanding to see what would happen. It could be used by all users from around the world, who could have many different views. This would be done through a statistical rather than knowledge-based technique since it would be learning from other data. Tay, the bot, started out with a completely unbiased view of the world, but based on the views of others, it would create biases and respond using them. However, it was assumed that the users who would interact with the bot would do so in a positive way. However, this was not the case seeing as the users did not have the goals Microsoft had hoped for. Users began to type terrible things to the bot because they thought it would be funny. The bot eventually started to say terribly racist, sexist, and overall awful things. This shows the ways in which a third party could break the system down in order to hurt others. Microsoft eventually had to take Tay down. If a project like this were to happen again, the company who did this should implement some knowledge-based rules that would prevent it from using certain keywords or phrases that some people could find offensive. However, Tay's failure could be an obstacle of artificial intelligence of its kind. After seeing what happened with Tay, other companies may not want to approach the same problem again. It would also likely be very difficult to determine what all users would find offensive and be very time-consuming and difficult to implement all of the different rules needed in order to stop this from happening. However, this did not stop many other users from trying the same feat but not necessarily with the same corpus. Some users have trained bots based on corpuses such as the speeches of presidential candidates, and others have tried the same as Microsoft without being met with the same negative inputs. Hopefully, Microsoft's failure does not mean the same will happen to others.